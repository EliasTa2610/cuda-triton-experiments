
==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22369 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==WARNING== Unable to access the following 6 metrics: ctc__rx_bytes_data_user.sum, ctc__rx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__rx_bytes_data_user.sum.per_second, ctc__tx_bytes_data_user.sum, ctc__tx_bytes_data_user.sum.pct_of_peak_sustained_elapsed, ctc__tx_bytes_data_user.sum.per_second.


==PROF== Profiling "convolution_var_2": Application replay pass 1
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22369

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22383 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 2
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22383

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22397 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 3
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22397

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22412 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 4
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22412

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22427 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 5
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22427

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22441 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 6
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22441

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22455 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 7
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22455

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22469 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 8
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22469

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22483 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 9
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22483

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22497 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 10
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22497

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22511 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 11
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22511

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22525 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 12
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22525

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22539 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 13
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22539

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22553 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 14
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22553

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22567 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 15
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22567

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22581 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 16
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22581

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22595 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 17
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22595

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22609 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 18
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22609

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22623 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 19
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22623

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22637 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 20
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22637

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22653 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 21
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22653

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22667 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 22
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22667

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22681 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 23
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22681

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22695 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 24
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22695

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22709 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 25
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22709

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22723 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 26
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22723

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22737 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 27
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22737

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22751 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 28
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22751

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22767 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 29
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22767

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22783 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 30
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22783

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22797 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 31
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22797

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22811 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 32
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22811

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22825 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 33
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22825

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22841 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 34
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22841

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22857 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 35
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22857

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22871 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 36
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22871

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22885 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 37
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22885

==ERROR== An error was reported by the driver:
==ERROR== Cuda driver is not compatible with Nsight Compute.
==PROF== Connected to process 22899 (/home/sesterce/conv)

==WARNING== An error was reported by the driver:
==WARNING== Failed to load Nsight Compute CUDA modules.
==PROF== Profiling "convolution_var_2": Application replay pass 38
var_1: P[0,0] = 0.360000
var_2: P[0,0] = 0.360000
var_3: P[0,0] = 0.360000
==PROF== Disconnected from process 22899
==PROF== Creating report from application replay data: 0%.....................100%
[22369] conv@127.0.0.1
  convolution_var_2(const float *, float *, int, int) (37, 37, 1)x(28, 28, 1), Context 1, Stream 7, Device 0, CC 9.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.10
    Elapsed Cycles                cycle        38206
    Memory Throughput                 %        53.43
    DRAM Throughput                   %         5.97
    Duration                         us        34.56
    L1/TEX Cache Throughput           %        61.25
    L2 Cache Throughput               %        15.78
    SM Active Cycles              cycle     33354.55
    Compute (SM) Throughput           %        50.59
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 2:1. The workload achieved 5%   
          of this device's FP32 peak performance and 0% of its FP64 peak performance. See the Profiling Guide           
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        26.67
    Maximum Sampling Interval          us         1.50
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.31
    Executed Ipc Elapsed  inst/cycle         2.02
    Issue Slots Busy               %        50.59
    Issued Ipc Active     inst/cycle         2.32
    SM Busy                        %        50.59
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (43.7%) based on elapsed cycles in the workload, taking into account the 
          rates of its different instructions. It executes integer and logic operations. It is well-utilized, but       
          should not be a bottleneck.                                                                                   

    Section: Memory Workload Analysis
    -------------------------------------- ----------- ------------
    Metric Name                            Metric Unit Metric Value
    -------------------------------------- ----------- ------------
    Local Memory Spilling Requests                                0
    Local Memory Spilling Request Overhead           %            0
    Memory Throughput                          Gbyte/s       121.59
    Mem Busy                                         %        53.43
    Max Bandwidth                                    %        31.01
    L1/TEX Hit Rate                                  %        35.14
    L2 Persisting Size                           Mbyte         9.83
    L2 Compression Success Rate                      %            0
    L2 Compression Ratio                             %            0
    L2 Compression Input Sectors                sector       162508
    L2 Hit Rate                                      %        69.97
    Mem Pipes Busy                                   %        50.55
    -------------------------------------- ----------- ------------

    Section: Memory Workload Analysis Chart
    OPT   Est. Speedup: 4.324%                                                                                          
          Out of the 5200256.0 bytes sent to the L2 Compression unit only 0.00% were successfully compressed. To        
          increase this success rate, consider marking only those memory regions as compressible that contain the most  
          zero values and/or expose the most homogeneous values.                                                        

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 27.71%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.4 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.63%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 30.25%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 2.0 - way bank        
          conflict across all 845625 shared load requests.This results in 824852 bank conflicts,  which represent       
          49.38% of the overall 1670477 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.06%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 1.3 - way bank       
          conflict across all 191570 shared store requests.This results in 51900 bank conflicts,  which represent       
          21.32% of the overall 243470 wavefronts for shared stores. Check the Source Counters section for uncoalesced  
          shared stores.                                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        58.61
    Issued Warp Per Scheduler                        0.59
    No Eligible                            %        41.39
    Active Warps Per Scheduler          warp        11.29
    Eligible Warps Per Scheduler        warp         2.23
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 1.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this workload allocates an average    
          of 11.29 active warps per scheduler, but only an average of 2.23 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.26
    Warp Cycles Per Executed Instruction           cycle        19.33
    Avg. Active Threads Per Warp                                29.16
    Avg. Not Predicated Off Threads Per Warp                    28.81
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     19271.64
    Executed Instructions                           inst      8787870
    Avg. Issued Instructions Per Scheduler          inst     19344.39
    Issued Instructions                             inst      8821041
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   784
    Cluster Scheduling Policy                           PolicySpread
    Cluster Size                                                   0
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1369
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.10
    # SMs                                         SM             114
    Stack Size                                                  1024
    Threads                                   thread         1073296
    # TPCs                                                        57
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 2%                                                                                              
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 784    
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Max Active Clusters                 cluster            0
    Max Cluster Size                      block            8
    Overall GPU Occupancy                     %            0
    Cluster Occupancy                         %            0
    Block Limit Barriers                  block           32
    Block Limit SM                        block           32
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block            2
    Theoretical Active Warps per SM        warp           50
    Theoretical Occupancy                     %        78.12
    Achieved Occupancy                        %        70.57
    Achieved Active Warps Per SM           warp        45.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 21.88%                                                                                          
          The 12.50 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (78.1%) is limited by the number of required      
          registers, and the number of warps within each block.                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         3283
    Total DRAM Elapsed Cycles        cycle      2199552
    Average L1 Active Cycles         cycle     33354.55
    Total L1 Elapsed Cycles          cycle      4359106
    Average L2 Active Cycles         cycle     28624.11
    Total L2 Elapsed Cycles          cycle      3012960
    Average SM Active Cycles         cycle     33354.55
    Total SM Elapsed Cycles          cycle      4359106
    Average SMSP Active Cycles       cycle     33005.43
    Total SMSP Elapsed Cycles        cycle     17436424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.08
    Branch Instructions              inst       690198
    Branch Efficiency                   %        72.77
    Avg. Divergent Branches      branches       163.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 12.36%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 84240 excessive sectors (16% of the total 
          517824 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.    
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 39.41%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 854678 excessive wavefronts (45% of the   
          total 1891873 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

